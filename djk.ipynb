{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/peixiaoqi/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n  warnings.warn(msg, UserWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "(Epoch 100) Minibatch:   0%|          | 0/196 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28f371c409544137acbdddbf76135a94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import gpytorch\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "aug_trans = [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip()]\n",
    "common_trans = [transforms.ToTensor(), normalize]\n",
    "train_compose = transforms.Compose(aug_trans + common_trans)\n",
    "test_compose = transforms.Compose(common_trans)\n",
    "\n",
    "\n",
    "dataset = \"cifar10\"\n",
    "\n",
    "\n",
    "if ('CI' in os.environ):  # this is for running the notebook in our testing framework\n",
    "    train_set = torch.utils.data.TensorDataset(torch.randn(8, 3, 32, 32), torch.rand(8).round().long())\n",
    "    test_set = torch.utils.data.TensorDataset(torch.randn(4, 3, 32, 32), torch.rand(4).round().long())\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=2, shuffle=False)\n",
    "    num_classes = 2\n",
    "elif dataset == 'cifar10':\n",
    "    train_set = dset.CIFAR10('data', train=True, transform=train_compose, download=True)\n",
    "    test_set = dset.CIFAR10('data', train=False, transform=test_compose)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "    num_classes = 10\n",
    "elif dataset == 'cifar100':\n",
    "    train_set = dset.CIFAR100('data', train=True, transform=train_compose, download=True)\n",
    "    test_set = dset.CIFAR100('data', train=False, transform=test_compose)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "    num_classes = 100\n",
    "else:\n",
    "    raise RuntimeError('dataset must be one of \"cifar100\" or \"cifar10\"')\n",
    "\n",
    "from densenet import DenseNet\n",
    "\n",
    "class DenseNetFeatureExtractor(DenseNet):\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(features.size(0), -1)\n",
    "        return out\n",
    "\n",
    "feature_extractor = DenseNetFeatureExtractor(block_config=(6, 6, 6), num_classes=num_classes)\n",
    "num_features = feature_extractor.classifier.in_features\n",
    "\n",
    "\n",
    "class GaussianProcessLayer(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, num_dim, grid_bounds=(-10., 10.), grid_size=64):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=grid_size, batch_shape=torch.Size([num_dim])\n",
    "        )\n",
    "\n",
    "        # Our base variational strategy is a GridInterpolationVariationalStrategy,\n",
    "        # which places variational inducing points on a Grid\n",
    "        # We wrap it with a IndependentMultitaskVariationalStrategy so that our output is a vector-valued GP\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.GridInterpolationVariationalStrategy(\n",
    "                self, grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                variational_distribution=variational_distribution,\n",
    "            ), num_tasks=num_dim,\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
    "                    math.exp(-1), math.exp(1), sigma=0.1, transform=torch.exp\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.grid_bounds = grid_bounds\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "\n",
    "\n",
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, num_dim, grid_bounds=(-10., 10.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GaussianProcessLayer(num_dim=num_dim, grid_bounds=grid_bounds)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_dim = num_dim\n",
    "\n",
    "        # This module will scale the NN features so that they're nice values\n",
    "        self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(self.grid_bounds[0], self.grid_bounds[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = self.scale_to_bounds(features)\n",
    "        # This next line makes it so that we learn a GP for each feature\n",
    "        features = features.transpose(-1, -2).unsqueeze(-1)\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "\n",
    "model = DKLModel(feature_extractor, num_dim=num_features)\n",
    "likelihood = gpytorch.likelihoods.SoftmaxLikelihood(num_features=model.num_dim, num_classes=num_classes)\n",
    "\n",
    "\n",
    "# If you run this example without CUDA, I hope you like waiting!\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "\n",
    "n_epochs = 1\n",
    "lr = 0.1\n",
    "optimizer = SGD([\n",
    "    {'params': model.feature_extractor.parameters(), 'weight_decay': 1e-4},\n",
    "    {'params': model.gp_layer.hyperparameters(), 'lr': lr * 0.01},\n",
    "    {'params': model.gp_layer.variational_parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=lr, momentum=0.9, nesterov=True, weight_decay=0)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs], gamma=0.1)\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=len(train_loader.dataset))\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    minibatch_iter = tqdm.notebook.tqdm(train_loader, desc=f\"(Epoch {epoch}) Minibatch\")\n",
    "    with gpytorch.settings.num_likelihood_samples(8):\n",
    "        for data, target in minibatch_iter:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = -mll(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    correct = 0\n",
    "    with torch.no_grad(), gpytorch.settings.num_likelihood_samples(16):\n",
    "        for data, target in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = likelihood(model(data))  # This gives us 16 samples from the predictive distribution\n",
    "            pred = output.probs.mean(0).argmax(-1)  # Taking the mean over all of the sample we've drawn\n",
    "            correct += pred.eq(target.view_as(pred)).cpu().sum()\n",
    "    print('Test set: Accuracy: {}/{} ({}%)'.format(\n",
    "        correct, len(test_loader.dataset), 100. * correct / float(len(test_loader.dataset))\n",
    "    ))\n",
    "\n",
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}